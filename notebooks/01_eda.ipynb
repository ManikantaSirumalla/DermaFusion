{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 01 - Comprehensive EDA: ISIC 2018 Task 3 / HAM10000\n\n**Objective:** Perform a thorough exploratory data analysis of the HAM10000 dermoscopic image dataset to inform modeling decisions for skin lesion classification.\n\n---\n\n## Table of Contents\n\n1. **Setup & Data Loading**\n2. **Dataset Overview & Schema Inspection**\n3. **Class Distribution & Imbalance Analysis**\n4. **Demographic Analysis** (Age, Sex)\n5. **Lesion Localization Analysis**\n6. **Missing Value Analysis**\n7. **Lesion Grouping & Data Leakage Risk**\n8. **Feature Correlations & Statistical Tests**\n9. **Image Quality & Properties Analysis**\n10. **Per-Class Sample Image Grid**\n11. **Co-occurrence & Interaction Analysis**\n12. **Class Imbalance Strategy Recommendations**\n13. **Key Findings & Modeling Implications**\n\n---\n\n**Dataset:** ISIC 2018 Challenge Task 3 (HAM10000)\n**Classes:** MEL (Melanoma), NV (Nevus), BCC (Basal Cell Carcinoma), AKIEC (Actinic Keratosis), BKL (Benign Keratosis), DF (Dermatofibroma), VASC (Vascular Lesion)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# =============================================================================\n# 1. Setup & Data Loading\n# =============================================================================\nfrom __future__ import annotations\n\nimport warnings\nfrom pathlib import Path\nfrom collections import Counter\n\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom PIL import Image\nfrom scipy import stats\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# -- Plotting defaults --\nsns.set_theme(style=\"whitegrid\", font_scale=1.1)\nplt.rcParams.update({\n    \"figure.dpi\": 120,\n    \"savefig.dpi\": 150,\n    \"axes.titlesize\": 14,\n    \"axes.labelsize\": 12,\n    \"figure.figsize\": (12, 5),\n})\n\n# Clinical label mapping for readable names\nLABEL_MAP = {\n    \"mel\": \"Melanoma\",\n    \"nv\": \"Melanocytic Nevus\",\n    \"bcc\": \"Basal Cell Carcinoma\",\n    \"akiec\": \"Actinic Keratosis\",\n    \"bkl\": \"Benign Keratosis\",\n    \"df\": \"Dermatofibroma\",\n    \"vasc\": \"Vascular Lesion\",\n}\nCLASS_ORDER = [\"nv\", \"mel\", \"bkl\", \"bcc\", \"akiec\", \"df\", \"vasc\"]\nPALETTE = sns.color_palette(\"Set2\", n_colors=7)\nCLASS_PALETTE = dict(zip(CLASS_ORDER, PALETTE))\n\n# -- Resolve paths --\nif Path(\"/content/DermaFusion\").exists():\n    PROJECT_ROOT = Path(\"/content/DermaFusion\")\nelse:\n    PROJECT_ROOT = Path.cwd().resolve().parents[0] if (Path.cwd() / \"src\").exists() is False else Path.cwd()\n\nRAW_DIR = PROJECT_ROOT / \"data\" / \"raw\"\nMETA_DIR = RAW_DIR / \"metadata\"\nMERGED_CSV = META_DIR / \"metadata_merged.csv\"\nHAM_META_CSV = META_DIR / \"HAM10000_metadata.csv\"\nTRAIN_GT = META_DIR / \"ISIC2018_Task3_Training_GroundTruth.csv\"\nGROUPINGS_CSV = META_DIR / \"ISIC2018_Task3_Training_LesionGroupings.csv\"\n\n# -- Load metadata --\nif MERGED_CSV.exists():\n    df = pd.read_csv(MERGED_CSV)\n    print(f\"Loaded merged metadata: {MERGED_CSV}\")\nelif HAM_META_CSV.exists():\n    df = pd.read_csv(HAM_META_CSV)\n    print(f\"Loaded HAM10000 metadata: {HAM_META_CSV}\")\nelif TRAIN_GT.exists() and GROUPINGS_CSV.exists():\n    gt = pd.read_csv(TRAIN_GT)\n    grp = pd.read_csv(GROUPINGS_CSV)\n    df = gt.merge(grp, on=\"image\", how=\"left\").rename(columns={\"image\": \"image_id\"})\n    print(f\"Built metadata from ISIC 2018 ground truth + lesion groupings\")\nelse:\n    raise FileNotFoundError(\n        \"No metadata found. Place metadata_merged.csv, HAM10000_metadata.csv, or \"\n        \"ISIC2018_Task3_Training_GroundTruth.csv + ISIC2018_Task3_Training_LesionGroupings.csv \"\n        \"in data/raw/metadata/.\"\n    )\n\n# -- Derive 'dx' column if only one-hot ground truth is available --\nif \"dx\" not in df.columns:\n    class_cols = [c for c in [\"MEL\", \"NV\", \"BCC\", \"AKIEC\", \"BKL\", \"DF\", \"VASC\"] if c in df.columns]\n    if class_cols:\n        df[\"dx\"] = df[class_cols].idxmax(axis=1).str.lower()\n\n# -- Resolve image directory --\nraw_images = RAW_DIR / \"images\"\npreprocessed = PROJECT_ROOT / \"data\" / \"preprocessed_hair_removed\" / \"images\"\nif (raw_images / \"train\").exists():\n    IMAGE_DIR = raw_images / \"train\"\nelif raw_images.exists() and list(raw_images.glob(\"*.jpg\")):\n    IMAGE_DIR = raw_images\nelif preprocessed.exists():\n    IMAGE_DIR = preprocessed\nelse:\n    IMAGE_DIR = raw_images\n\nprint(f\"Project root : {PROJECT_ROOT}\")\nprint(f\"Image dir    : {IMAGE_DIR}\")\nprint(f\"Total samples: {len(df):,}\")\nprint(f\"Columns      : {list(df.columns)}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# =============================================================================\n# 2. Dataset Overview & Schema Inspection\n# =============================================================================\nprint(\"=\" * 60)\nprint(\"DATASET SCHEMA\")\nprint(\"=\" * 60)\nprint(f\"\\nShape: {df.shape[0]:,} rows x {df.shape[1]} columns\\n\")\nprint(df.dtypes.to_string())\nprint(\"\\n\" + \"=\" * 60)\nprint(\"FIRST 5 ROWS\")\nprint(\"=\" * 60)\ndisplay(df.head())\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"DESCRIPTIVE STATISTICS (Numeric)\")\nprint(\"=\" * 60)\ndisplay(df.describe().round(2))\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"DESCRIPTIVE STATISTICS (Categorical)\")\nprint(\"=\" * 60)\ncat_cols = df.select_dtypes(include=[\"object\"]).columns.tolist()\nif cat_cols:\n    display(df[cat_cols].describe())\n\n# Unique value counts per column\nprint(\"\\n\" + \"=\" * 60)\nprint(\"UNIQUE VALUES PER COLUMN\")\nprint(\"=\" * 60)\nunique_counts = pd.DataFrame({\n    \"column\": df.columns,\n    \"dtype\": df.dtypes.values,\n    \"unique\": [df[c].nunique() for c in df.columns],\n    \"missing\": df.isna().sum().values,\n    \"missing_pct\": (df.isna().sum() / len(df) * 100).round(2).values,\n    \"sample_values\": [str(df[c].dropna().unique()[:5].tolist()) for c in df.columns],\n})\ndisplay(unique_counts)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# =============================================================================\n# 3. Class Distribution & Imbalance Analysis\n# =============================================================================\nclass_counts = df[\"dx\"].value_counts().reindex(CLASS_ORDER)\nclass_pct = (class_counts / class_counts.sum() * 100).round(2)\nmajority_class = class_counts.max()\nimbalance_ratio = (majority_class / class_counts).round(1)\n\nsummary = pd.DataFrame({\n    \"Class\": CLASS_ORDER,\n    \"Full Name\": [LABEL_MAP[c] for c in CLASS_ORDER],\n    \"Count\": class_counts.values,\n    \"Percent (%)\": class_pct.values,\n    \"Imbalance Ratio (vs majority)\": imbalance_ratio.values,\n})\ndisplay(summary)\n\nfig, axes = plt.subplots(1, 3, figsize=(20, 5))\n\n# Bar plot with counts and percentages\nbars = axes[0].bar(CLASS_ORDER, class_counts.values, color=[CLASS_PALETTE[c] for c in CLASS_ORDER],\n                   edgecolor=\"black\", linewidth=0.5)\nfor bar, count, pct in zip(bars, class_counts.values, class_pct.values):\n    axes[0].text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 30,\n                 f\"{count:,}\\n({pct}%)\", ha=\"center\", va=\"bottom\", fontsize=9, fontweight=\"bold\")\naxes[0].set_title(\"Class Distribution (Counts)\", fontweight=\"bold\")\naxes[0].set_ylabel(\"Number of Samples\")\naxes[0].set_xlabel(\"Diagnosis\")\n\n# Pie chart\naxes[1].pie(class_counts.values, labels=[LABEL_MAP[c] for c in CLASS_ORDER],\n            autopct=\"%1.1f%%\", colors=[CLASS_PALETTE[c] for c in CLASS_ORDER],\n            startangle=140, pctdistance=0.8, textprops={\"fontsize\": 8})\naxes[1].set_title(\"Class Proportions\", fontweight=\"bold\")\n\n# Imbalance ratio bar (log scale)\nbars2 = axes[2].bar(CLASS_ORDER, imbalance_ratio.values, color=[CLASS_PALETTE[c] for c in CLASS_ORDER],\n                    edgecolor=\"black\", linewidth=0.5)\nfor bar, ratio in zip(bars2, imbalance_ratio.values):\n    axes[2].text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.2,\n                 f\"{ratio}x\", ha=\"center\", va=\"bottom\", fontsize=9, fontweight=\"bold\")\naxes[2].set_yscale(\"log\")\naxes[2].set_title(\"Imbalance Ratio (Majority / Class)\", fontweight=\"bold\")\naxes[2].set_ylabel(\"Ratio (log scale)\")\naxes[2].set_xlabel(\"Diagnosis\")\naxes[2].axhline(y=1, color=\"gray\", linestyle=\"--\", alpha=0.5)\n\nplt.tight_layout()\nplt.show()\n\n# Effective number of samples (for class-balanced loss)\nbeta = 0.9999\neffective_n = (1 - beta ** class_counts.values) / (1 - beta)\ninv_freq_weights = (1.0 / class_counts.values)\ninv_freq_weights = inv_freq_weights / inv_freq_weights.sum() * len(CLASS_ORDER)\n\nweight_df = pd.DataFrame({\n    \"Class\": CLASS_ORDER,\n    \"Count\": class_counts.values,\n    \"Inverse-Freq Weight\": inv_freq_weights.round(4),\n    \"Effective Samples (beta=0.9999)\": effective_n.round(1),\n})\nprint(\"\\nClass weighting reference (for loss functions):\")\ndisplay(weight_df)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# =============================================================================\n# 4. Demographic Analysis (Age & Sex)\n# =============================================================================\n\nfig = plt.figure(figsize=(22, 18))\ngs = gridspec.GridSpec(3, 3, hspace=0.35, wspace=0.3)\n\n# --- 4a. Overall age distribution ---\nif \"age\" in df.columns:\n    ax1 = fig.add_subplot(gs[0, 0])\n    df[\"age\"].dropna().hist(bins=30, ax=ax1, color=\"steelblue\", edgecolor=\"black\", alpha=0.8)\n    ax1.axvline(df[\"age\"].median(), color=\"red\", linestyle=\"--\", linewidth=1.5, label=f\"Median: {df['age'].median():.0f}\")\n    ax1.axvline(df[\"age\"].mean(), color=\"orange\", linestyle=\"--\", linewidth=1.5, label=f\"Mean: {df['age'].mean():.1f}\")\n    ax1.set_title(\"Overall Age Distribution\", fontweight=\"bold\")\n    ax1.set_xlabel(\"Age\")\n    ax1.set_ylabel(\"Count\")\n    ax1.legend(fontsize=9)\n\n    # --- 4b. Age by diagnosis (box + violin) ---\n    ax2 = fig.add_subplot(gs[0, 1:])\n    order = CLASS_ORDER\n    sns.violinplot(data=df, x=\"dx\", y=\"age\", order=order, palette=CLASS_PALETTE,\n                   inner=None, alpha=0.3, ax=ax2)\n    sns.boxplot(data=df, x=\"dx\", y=\"age\", order=order, palette=CLASS_PALETTE,\n                width=0.3, boxprops=dict(alpha=0.8), ax=ax2)\n    ax2.set_title(\"Age Distribution by Diagnosis (Violin + Box)\", fontweight=\"bold\")\n    ax2.set_xlabel(\"Diagnosis\")\n    ax2.set_ylabel(\"Age\")\n\n    # --- 4c. Age density per class (overlaid KDE) ---\n    ax3 = fig.add_subplot(gs[1, 0:2])\n    for cls in CLASS_ORDER:\n        subset = df.loc[df[\"dx\"] == cls, \"age\"].dropna()\n        if len(subset) > 1:\n            subset.plot.kde(ax=ax3, label=f\"{cls.upper()} (n={len(subset)})\",\n                            color=CLASS_PALETTE[cls], linewidth=1.5)\n    ax3.set_title(\"Age Density per Diagnosis (KDE)\", fontweight=\"bold\")\n    ax3.set_xlabel(\"Age\")\n    ax3.set_ylabel(\"Density\")\n    ax3.legend(fontsize=8, ncol=2)\n    ax3.set_xlim(0, 100)\n\n    # --- 4d. Age statistics table ---\n    ax4 = fig.add_subplot(gs[1, 2])\n    ax4.axis(\"off\")\n    age_stats = df.groupby(\"dx\")[\"age\"].agg([\"count\", \"mean\", \"median\", \"std\", \"min\", \"max\"]).reindex(CLASS_ORDER).round(1)\n    age_stats.columns = [\"N\", \"Mean\", \"Median\", \"Std\", \"Min\", \"Max\"]\n    tbl = ax4.table(cellText=age_stats.values, rowLabels=age_stats.index,\n                    colLabels=age_stats.columns, loc=\"center\", cellLoc=\"center\")\n    tbl.auto_set_font_size(False)\n    tbl.set_fontsize(9)\n    tbl.scale(1.2, 1.4)\n    ax4.set_title(\"Age Statistics by Class\", fontweight=\"bold\", pad=20)\n\n# --- 4e. Sex distribution (overall) ---\nif \"sex\" in df.columns:\n    ax5 = fig.add_subplot(gs[2, 0])\n    sex_counts = df[\"sex\"].value_counts()\n    ax5.pie(sex_counts.values, labels=sex_counts.index, autopct=\"%1.1f%%\",\n            colors=sns.color_palette(\"Pastel1\", n_colors=len(sex_counts)),\n            startangle=90, textprops={\"fontsize\": 10})\n    ax5.set_title(\"Overall Sex Distribution\", fontweight=\"bold\")\n\n    # --- 4f. Sex by diagnosis (stacked %) ---\n    ax6 = fig.add_subplot(gs[2, 1])\n    sex_dx = pd.crosstab(df[\"dx\"], df[\"sex\"], normalize=\"index\").reindex(CLASS_ORDER) * 100\n    sex_dx.plot(kind=\"bar\", stacked=True, ax=ax6, colormap=\"Pastel1\", edgecolor=\"black\", linewidth=0.5)\n    ax6.set_title(\"Sex Distribution by Diagnosis (%)\", fontweight=\"bold\")\n    ax6.set_ylabel(\"Percent\")\n    ax6.set_xlabel(\"Diagnosis\")\n    ax6.legend(title=\"Sex\", fontsize=8)\n    ax6.tick_params(axis=\"x\", rotation=30)\n\n    # --- 4g. Sex by diagnosis (grouped counts) ---\n    ax7 = fig.add_subplot(gs[2, 2])\n    sex_dx_counts = pd.crosstab(df[\"dx\"], df[\"sex\"]).reindex(CLASS_ORDER)\n    sex_dx_counts.plot(kind=\"bar\", ax=ax7, colormap=\"Set2\", edgecolor=\"black\", linewidth=0.5)\n    ax7.set_title(\"Sex by Diagnosis (Absolute Counts)\", fontweight=\"bold\")\n    ax7.set_ylabel(\"Count\")\n    ax7.set_xlabel(\"Diagnosis\")\n    ax7.legend(title=\"Sex\", fontsize=8)\n    ax7.tick_params(axis=\"x\", rotation=30)\n\nplt.suptitle(\"Section 4: Demographic Analysis\", fontsize=16, fontweight=\"bold\", y=1.01)\nplt.show()\n\n# --- 4h. Age distribution comparison: Male vs Female per class ---\nif \"age\" in df.columns and \"sex\" in df.columns:\n    fig, axes = plt.subplots(2, 4, figsize=(20, 8))\n    axes = axes.flatten()\n    for idx, cls in enumerate(CLASS_ORDER):\n        ax = axes[idx]\n        for sex_val in [\"male\", \"female\"]:\n            subset = df[(df[\"dx\"] == cls) & (df[\"sex\"] == sex_val)][\"age\"].dropna()\n            if len(subset) > 1:\n                subset.plot.kde(ax=ax, label=sex_val.capitalize(), linewidth=1.5)\n        ax.set_title(f\"{cls.upper()}\", fontweight=\"bold\", fontsize=11)\n        ax.set_xlabel(\"Age\")\n        ax.legend(fontsize=8)\n        ax.set_xlim(0, 100)\n    # Remove unused subplot\n    if len(CLASS_ORDER) < len(axes):\n        for i in range(len(CLASS_ORDER), len(axes)):\n            axes[i].axis(\"off\")\n    plt.suptitle(\"Age Distribution: Male vs Female per Diagnosis\", fontsize=14, fontweight=\"bold\")\n    plt.tight_layout()\n    plt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# =============================================================================\n# 5. Lesion Localization Analysis\n# =============================================================================\nif \"localization\" in df.columns:\n    fig, axes = plt.subplots(2, 2, figsize=(22, 14))\n\n    # --- 5a. Overall localization distribution ---\n    loc_counts = df[\"localization\"].value_counts().head(15)\n    sns.barplot(x=loc_counts.values, y=loc_counts.index, palette=\"viridis\", ax=axes[0, 0],\n                edgecolor=\"black\", linewidth=0.5)\n    for i, (val, name) in enumerate(zip(loc_counts.values, loc_counts.index)):\n        axes[0, 0].text(val + 10, i, f\"{val:,} ({val / len(df) * 100:.1f}%)\", va=\"center\", fontsize=9)\n    axes[0, 0].set_title(\"Top 15 Lesion Localizations\", fontweight=\"bold\")\n    axes[0, 0].set_xlabel(\"Count\")\n\n    # --- 5b. Localization heatmap by diagnosis ---\n    loc_dx = pd.crosstab(df[\"dx\"], df[\"localization\"]).reindex(CLASS_ORDER)\n    sns.heatmap(loc_dx, cmap=\"YlOrRd\", annot=True, fmt=\"d\", linewidths=0.5,\n                ax=axes[0, 1], cbar_kws={\"shrink\": 0.8})\n    axes[0, 1].set_title(\"Diagnosis vs Localization (Counts)\", fontweight=\"bold\")\n    axes[0, 1].tick_params(axis=\"x\", rotation=45)\n\n    # --- 5c. Normalized heatmap (row-normalized to show % within each class) ---\n    loc_dx_pct = pd.crosstab(df[\"dx\"], df[\"localization\"], normalize=\"index\").reindex(CLASS_ORDER) * 100\n    sns.heatmap(loc_dx_pct, cmap=\"YlGnBu\", annot=True, fmt=\".1f\", linewidths=0.5,\n                ax=axes[1, 0], cbar_kws={\"shrink\": 0.8, \"label\": \"% within class\"})\n    axes[1, 0].set_title(\"Localization Distribution within Each Class (%)\", fontweight=\"bold\")\n    axes[1, 0].tick_params(axis=\"x\", rotation=45)\n\n    # --- 5d. Top localization per class ---\n    top_loc_per_class = []\n    for cls in CLASS_ORDER:\n        cls_locs = df[df[\"dx\"] == cls][\"localization\"].value_counts()\n        if len(cls_locs) > 0:\n            top_loc_per_class.append({\n                \"Class\": cls.upper(),\n                \"Top 1\": f\"{cls_locs.index[0]} ({cls_locs.iloc[0]:,})\",\n                \"Top 2\": f\"{cls_locs.index[1]} ({cls_locs.iloc[1]:,})\" if len(cls_locs) > 1 else \"-\",\n                \"Top 3\": f\"{cls_locs.index[2]} ({cls_locs.iloc[2]:,})\" if len(cls_locs) > 2 else \"-\",\n                \"Unique Sites\": cls_locs.shape[0],\n            })\n    axes[1, 1].axis(\"off\")\n    tbl = axes[1, 1].table(\n        cellText=[list(d.values()) for d in top_loc_per_class],\n        colLabels=list(top_loc_per_class[0].keys()),\n        loc=\"center\", cellLoc=\"center\",\n    )\n    tbl.auto_set_font_size(False)\n    tbl.set_fontsize(9)\n    tbl.scale(1.3, 1.5)\n    axes[1, 1].set_title(\"Top Localizations per Class\", fontweight=\"bold\", pad=20)\n\n    plt.suptitle(\"Section 5: Localization Analysis\", fontsize=16, fontweight=\"bold\", y=1.01)\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"'localization' column not found - skipping localization analysis.\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# =============================================================================\n# 6. Missing Value Analysis\n# =============================================================================\nfig, axes = plt.subplots(1, 3, figsize=(22, 6))\n\n# --- 6a. Overall missing value bar chart ---\nmissing_abs = df.isna().sum().sort_values(ascending=False)\nmissing_pct = (missing_abs / len(df) * 100).round(2)\nmissing_df = pd.DataFrame({\"count\": missing_abs, \"pct\": missing_pct})\nmissing_present = missing_df[missing_df[\"count\"] > 0]\n\nif len(missing_present) > 0:\n    bars = axes[0].barh(missing_present.index, missing_present[\"pct\"], color=\"salmon\", edgecolor=\"black\")\n    for bar, pct, cnt in zip(bars, missing_present[\"pct\"], missing_present[\"count\"]):\n        axes[0].text(bar.get_width() + 0.3, bar.get_y() + bar.get_height() / 2,\n                     f\"{pct}% ({cnt:,})\", va=\"center\", fontsize=9)\n    axes[0].set_title(\"Missing Values by Column\", fontweight=\"bold\")\n    axes[0].set_xlabel(\"% Missing\")\nelse:\n    axes[0].text(0.5, 0.5, \"No missing values!\", ha=\"center\", va=\"center\", fontsize=14)\n    axes[0].set_title(\"Missing Values by Column\", fontweight=\"bold\")\n\n# --- 6b. Missing value heatmap (sample of rows) ---\nsample_idx = np.random.RandomState(42).choice(len(df), min(200, len(df)), replace=False)\ncols_of_interest = [c for c in [\"age\", \"sex\", \"localization\", \"dx\", \"lesion_id\", \"dx_type\"] if c in df.columns]\nif cols_of_interest:\n    sns.heatmap(df.iloc[sample_idx][cols_of_interest].isna().astype(int).T,\n                cmap=\"YlOrRd\", cbar_kws={\"label\": \"Missing (1) / Present (0)\"},\n                ax=axes[1], yticklabels=True)\n    axes[1].set_title(\"Missing Pattern (200-row sample)\", fontweight=\"bold\")\n    axes[1].set_xlabel(\"Sample Index\")\n\n# --- 6c. Missingness by class ---\nif \"age\" in df.columns:\n    miss_by_class = df.groupby(\"dx\")[cols_of_interest].apply(lambda x: x.isna().mean() * 100).round(1)\n    if hasattr(miss_by_class, \"reindex\"):\n        miss_by_class = miss_by_class.reindex(CLASS_ORDER)\n    sns.heatmap(miss_by_class, annot=True, fmt=\".1f\", cmap=\"Oranges\", linewidths=0.5,\n                ax=axes[2], cbar_kws={\"label\": \"% Missing\"})\n    axes[2].set_title(\"Missing % by Class\", fontweight=\"bold\")\n\nplt.suptitle(\"Section 6: Missing Value Analysis\", fontsize=16, fontweight=\"bold\", y=1.02)\nplt.tight_layout()\nplt.show()\n\n# Summary table\nprint(\"\\nMissing Value Summary:\")\ndisplay(missing_df[missing_df[\"count\"] > 0].rename(columns={\"count\": \"Missing Count\", \"pct\": \"Missing %\"}))",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# =============================================================================\n# 7. Lesion Grouping & Data Leakage Risk Analysis\n# =============================================================================\nif \"lesion_id\" in df.columns:\n    lesion_counts = df[\"lesion_id\"].value_counts()\n    n_unique_lesions = lesion_counts.shape[0]\n    n_images = len(df)\n    multi_image_lesions = (lesion_counts > 1).sum()\n    max_images_per_lesion = lesion_counts.max()\n\n    print(f\"Total images          : {n_images:,}\")\n    print(f\"Unique lesions        : {n_unique_lesions:,}\")\n    print(f\"Images / lesion ratio : {n_images / n_unique_lesions:.2f}\")\n    print(f\"Lesions with >1 image : {multi_image_lesions:,} ({multi_image_lesions / n_unique_lesions * 100:.1f}%)\")\n    print(f\"Max images per lesion : {max_images_per_lesion}\")\n\n    fig, axes = plt.subplots(1, 3, figsize=(22, 5))\n\n    # --- 7a. Images per lesion histogram ---\n    sns.histplot(lesion_counts, bins=range(1, lesion_counts.max() + 2), ax=axes[0],\n                 color=\"steelblue\", edgecolor=\"black\")\n    axes[0].set_title(\"Images per Lesion Distribution\", fontweight=\"bold\")\n    axes[0].set_xlabel(\"Images per Lesion\")\n    axes[0].set_ylabel(\"Number of Lesions\")\n\n    # --- 7b. Duplicate lesion breakdown by class ---\n    lesion_class = df.groupby(\"lesion_id\")[\"dx\"].first()\n    multi_lesion_ids = lesion_counts[lesion_counts > 1].index\n    multi_lesion_classes = lesion_class.loc[multi_lesion_ids].value_counts().reindex(CLASS_ORDER, fill_value=0)\n    single_lesion_ids = lesion_counts[lesion_counts == 1].index\n    single_lesion_classes = lesion_class.loc[single_lesion_ids].value_counts().reindex(CLASS_ORDER, fill_value=0)\n\n    x = np.arange(len(CLASS_ORDER))\n    width = 0.35\n    axes[1].bar(x - width / 2, single_lesion_classes.values, width, label=\"Single-image lesions\",\n                color=\"skyblue\", edgecolor=\"black\", linewidth=0.5)\n    axes[1].bar(x + width / 2, multi_lesion_classes.values, width, label=\"Multi-image lesions\",\n                color=\"coral\", edgecolor=\"black\", linewidth=0.5)\n    axes[1].set_xticks(x)\n    axes[1].set_xticklabels([c.upper() for c in CLASS_ORDER])\n    axes[1].set_title(\"Single vs Multi-Image Lesions by Class\", fontweight=\"bold\")\n    axes[1].set_ylabel(\"Number of Lesions\")\n    axes[1].legend()\n\n    # --- 7c. Data leakage risk: what % of images share a lesion with another ---\n    images_in_multi = df[df[\"lesion_id\"].isin(multi_lesion_ids)].shape[0]\n    images_in_single = n_images - images_in_multi\n    axes[2].pie([images_in_single, images_in_multi],\n                labels=[\"Unique lesion images\", \"Shared lesion images\"],\n                autopct=\"%1.1f%%\", colors=[\"#66b3ff\", \"#ff6666\"],\n                startangle=90, textprops={\"fontsize\": 11})\n    axes[2].set_title(\"Data Leakage Risk: Shared Lesion Images\", fontweight=\"bold\")\n\n    plt.suptitle(\"Section 7: Lesion Grouping & Leakage Risk\", fontsize=16, fontweight=\"bold\", y=1.02)\n    plt.tight_layout()\n    plt.show()\n\n    print(\"\\nKey insight: Lesion-level splitting is REQUIRED to prevent data leakage.\")\n    print(f\"If images from the same lesion appear in both train and val/test,\")\n    print(f\"the model will memorize lesion appearance rather than learning diagnostic features.\")\n    print(f\"Affected images: {images_in_multi:,} ({images_in_multi / n_images * 100:.1f}% of dataset)\")\nelse:\n    print(\"'lesion_id' column not found - skipping lesion grouping analysis.\")"
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# 8. Feature Correlations & Statistical Tests\n# =============================================================================\n\n# --- 8a. Numeric metadata correlation matrix ---\nnumeric_cols = [c for c in df.columns if df[c].dtype in [\"float64\", \"float32\", \"int64\", \"int32\"]\n                and c not in [\"MEL\", \"NV\", \"BCC\", \"AKIEC\", \"BKL\", \"DF\", \"VASC\"]]\n\nfig, axes = plt.subplots(1, 2, figsize=(18, 6))\n\nif len(numeric_cols) >= 2:\n    corr = df[numeric_cols].corr()\n    mask = np.triu(np.ones_like(corr, dtype=bool))\n    sns.heatmap(corr, mask=mask, annot=True, fmt=\".2f\", cmap=\"RdBu_r\", vmin=-1, vmax=1,\n                linewidths=0.5, ax=axes[0], square=True)\n    axes[0].set_title(\"Numeric Feature Correlations\", fontweight=\"bold\")\nelse:\n    axes[0].text(0.5, 0.5, \"Not enough numeric columns for correlation\", ha=\"center\", va=\"center\")\n    axes[0].set_title(\"Numeric Feature Correlations\", fontweight=\"bold\")\n\n# --- 8b. Cramér's V for categorical associations ---\ndef cramers_v(x, y):\n    \"\"\"Cramér's V statistic for categorical-categorical association.\"\"\"\n    confusion_matrix = pd.crosstab(x, y)\n    chi2 = stats.chi2_contingency(confusion_matrix)[0]\n    n = confusion_matrix.sum().sum()\n    min_dim = min(confusion_matrix.shape) - 1\n    if min_dim == 0 or n == 0:\n        return 0.0\n    return np.sqrt(chi2 / (n * min_dim))\n\ncat_features = [c for c in [\"dx\", \"sex\", \"localization\", \"dx_type\"] if c in df.columns]\nif len(cat_features) >= 2:\n    cv_matrix = pd.DataFrame(index=cat_features, columns=cat_features, dtype=float)\n    for c1 in cat_features:\n        for c2 in cat_features:\n            valid = df[[c1, c2]].dropna()\n            cv_matrix.loc[c1, c2] = cramers_v(valid[c1], valid[c2]) if len(valid) > 0 else 0.0\n    cv_matrix = cv_matrix.astype(float)\n    sns.heatmap(cv_matrix, annot=True, fmt=\".3f\", cmap=\"Purples\", vmin=0, vmax=1,\n                linewidths=0.5, ax=axes[1], square=True)\n    axes[1].set_title(\"Cramér's V (Categorical Associations)\", fontweight=\"bold\")\nelse:\n    axes[1].text(0.5, 0.5, \"Not enough categorical columns\", ha=\"center\", va=\"center\")\n    axes[1].set_title(\"Cramér's V (Categorical Associations)\", fontweight=\"bold\")\n\nplt.suptitle(\"Section 8: Feature Correlations & Statistical Tests\", fontsize=16, fontweight=\"bold\", y=1.02)\nplt.tight_layout()\nplt.show()\n\n# --- 8c. ANOVA: Is age significantly different across diagnoses? ---\nif \"age\" in df.columns:\n    groups = [group[\"age\"].dropna().values for _, group in df.groupby(\"dx\")]\n    groups = [g for g in groups if len(g) > 1]\n    if len(groups) >= 2:\n        f_stat, p_val = stats.f_oneway(*groups)\n        print(f\"\\nOne-way ANOVA (Age ~ Diagnosis):\")\n        print(f\"  F-statistic = {f_stat:.4f}\")\n        print(f\"  p-value     = {p_val:.2e}\")\n        print(f\"  Significant = {'YES' if p_val < 0.05 else 'NO'} (alpha=0.05)\")\n\n# --- 8d. Chi-Square test: Sex vs Diagnosis ---\nif \"sex\" in df.columns:\n    contingency = pd.crosstab(df[\"dx\"], df[\"sex\"])\n    chi2, p_val, dof, expected = stats.chi2_contingency(contingency)\n    print(f\"\\nChi-Square Test (Sex x Diagnosis):\")\n    print(f\"  Chi2       = {chi2:.4f}\")\n    print(f\"  p-value    = {p_val:.2e}\")\n    print(f\"  DoF        = {dof}\")\n    print(f\"  Significant = {'YES' if p_val < 0.05 else 'NO'} (alpha=0.05)\")\n\n# --- 8e. Chi-Square test: Localization vs Diagnosis ---\nif \"localization\" in df.columns:\n    contingency_loc = pd.crosstab(df[\"dx\"], df[\"localization\"])\n    chi2_loc, p_val_loc, dof_loc, _ = stats.chi2_contingency(contingency_loc)\n    print(f\"\\nChi-Square Test (Localization x Diagnosis):\")\n    print(f\"  Chi2       = {chi2_loc:.4f}\")\n    print(f\"  p-value    = {p_val_loc:.2e}\")\n    print(f\"  DoF        = {dof_loc}\")\n    print(f\"  Significant = {'YES' if p_val_loc < 0.05 else 'NO'} (alpha=0.05)\")\n\nprint(\"\\nInterpretation: Significant results confirm metadata features carry diagnostic signal,\"\n      \"\\nsupporting the multi-modal approach (image + metadata).\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# 9. Image Quality & Properties Analysis\n# =============================================================================\nimage_files = sorted(IMAGE_DIR.glob(\"*.jpg\"))\nif not image_files:\n    # Try subdirectories\n    for sub in [\"train\", \"val\", \"test\"]:\n        image_files.extend(sorted((IMAGE_DIR / sub).glob(\"*.jpg\")) if (IMAGE_DIR / sub).exists() else [])\n    if not image_files:\n        image_files = sorted(IMAGE_DIR.glob(\"**/*.jpg\"))\n\nn_sample = min(500, len(image_files))\nrng = np.random.RandomState(42)\nsampled_files = rng.choice(image_files, n_sample, replace=False) if len(image_files) > n_sample else image_files\n\nprint(f\"Analyzing {len(sampled_files)} images (sampled from {len(image_files)} total)...\")\n\nimg_stats = []\nfor path in sampled_files:\n    with Image.open(path) as img:\n        w, h = img.size\n        arr = np.asarray(img.convert(\"RGB\"), dtype=np.float32)\n        img_stats.append({\n            \"file\": path.stem,\n            \"width\": w,\n            \"height\": h,\n            \"aspect_ratio\": round(w / h, 3),\n            \"mean_r\": arr[:, :, 0].mean(),\n            \"mean_g\": arr[:, :, 1].mean(),\n            \"mean_b\": arr[:, :, 2].mean(),\n            \"std_r\": arr[:, :, 0].std(),\n            \"std_g\": arr[:, :, 1].std(),\n            \"std_b\": arr[:, :, 2].std(),\n            \"brightness\": arr.mean(),\n            \"contrast\": arr.std(),\n            \"variance\": arr.var(),\n        })\n\nimg_df = pd.DataFrame(img_stats)\nprint(f\"\\nImage statistics summary (n={len(img_df)}):\")\ndisplay(img_df[[\"width\", \"height\", \"aspect_ratio\", \"brightness\", \"contrast\"]].describe().round(2))\n\nfig, axes = plt.subplots(2, 3, figsize=(22, 12))\n\n# --- 9a. Width/Height scatter ---\naxes[0, 0].scatter(img_df[\"width\"], img_df[\"height\"], alpha=0.3, s=10, color=\"steelblue\")\naxes[0, 0].set_title(\"Image Dimensions (Width x Height)\", fontweight=\"bold\")\naxes[0, 0].set_xlabel(\"Width (px)\")\naxes[0, 0].set_ylabel(\"Height (px)\")\naxes[0, 0].set_aspect(\"equal\")\n\n# --- 9b. Aspect ratio distribution ---\nsns.histplot(img_df[\"aspect_ratio\"], bins=50, ax=axes[0, 1], color=\"teal\", edgecolor=\"black\")\naxes[0, 1].axvline(1.0, color=\"red\", linestyle=\"--\", label=\"Square (1:1)\")\naxes[0, 1].set_title(\"Aspect Ratio Distribution\", fontweight=\"bold\")\naxes[0, 1].set_xlabel(\"Aspect Ratio (W/H)\")\naxes[0, 1].legend()\n\n# --- 9c. Mean color channel distributions ---\nfor ch, col, name in zip([\"mean_r\", \"mean_g\", \"mean_b\"], [\"red\", \"green\", \"blue\"], [\"R\", \"G\", \"B\"]):\n    sns.kdeplot(img_df[ch], ax=axes[0, 2], color=col, label=name, linewidth=1.5, fill=True, alpha=0.2)\naxes[0, 2].set_title(\"Mean Color Channel Distribution\", fontweight=\"bold\")\naxes[0, 2].set_xlabel(\"Mean Pixel Value (0-255)\")\naxes[0, 2].legend()\n\n# --- 9d. Brightness distribution ---\nsns.histplot(img_df[\"brightness\"], bins=40, ax=axes[1, 0], color=\"gold\", edgecolor=\"black\")\naxes[1, 0].axvline(img_df[\"brightness\"].mean(), color=\"red\", linestyle=\"--\",\n                    label=f\"Mean: {img_df['brightness'].mean():.1f}\")\naxes[1, 0].set_title(\"Image Brightness Distribution\", fontweight=\"bold\")\naxes[1, 0].set_xlabel(\"Mean Pixel Intensity\")\naxes[1, 0].legend()\n\n# --- 9e. Contrast distribution ---\nsns.histplot(img_df[\"contrast\"], bins=40, ax=axes[1, 1], color=\"mediumpurple\", edgecolor=\"black\")\naxes[1, 1].axvline(img_df[\"contrast\"].mean(), color=\"red\", linestyle=\"--\",\n                    label=f\"Mean: {img_df['contrast'].mean():.1f}\")\naxes[1, 1].set_title(\"Image Contrast (Std Dev) Distribution\", fontweight=\"bold\")\naxes[1, 1].set_xlabel(\"Pixel Std Dev\")\naxes[1, 1].legend()\n\n# --- 9f. Brightness vs Contrast scatter ---\naxes[1, 2].scatter(img_df[\"brightness\"], img_df[\"contrast\"], alpha=0.3, s=10, color=\"darkorange\")\naxes[1, 2].set_title(\"Brightness vs Contrast\", fontweight=\"bold\")\naxes[1, 2].set_xlabel(\"Brightness (Mean Intensity)\")\naxes[1, 2].set_ylabel(\"Contrast (Std Dev)\")\n\nplt.suptitle(\"Section 9: Image Quality & Properties\", fontsize=16, fontweight=\"bold\", y=1.01)\nplt.tight_layout()\nplt.show()\n\n# --- 9g. Per-class brightness/contrast comparison ---\nif \"dx\" in df.columns and \"image_id\" in df.columns:\n    img_df_merged = img_df.merge(df[[\"image_id\", \"dx\"]].rename(columns={\"image_id\": \"file\"}),\n                                  on=\"file\", how=\"inner\")\n    if len(img_df_merged) > 20:\n        fig, axes = plt.subplots(1, 2, figsize=(18, 5))\n        sns.boxplot(data=img_df_merged, x=\"dx\", y=\"brightness\", order=CLASS_ORDER,\n                    palette=CLASS_PALETTE, ax=axes[0])\n        axes[0].set_title(\"Brightness by Diagnosis\", fontweight=\"bold\")\n        axes[0].set_xlabel(\"Diagnosis\")\n        axes[0].set_ylabel(\"Mean Pixel Intensity\")\n\n        sns.boxplot(data=img_df_merged, x=\"dx\", y=\"contrast\", order=CLASS_ORDER,\n                    palette=CLASS_PALETTE, ax=axes[1])\n        axes[1].set_title(\"Contrast by Diagnosis\", fontweight=\"bold\")\n        axes[1].set_xlabel(\"Diagnosis\")\n        axes[1].set_ylabel(\"Pixel Std Dev\")\n\n        plt.suptitle(\"Image Properties by Diagnosis\", fontsize=14, fontweight=\"bold\", y=1.02)\n        plt.tight_layout()\n        plt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# 10. Per-Class Sample Image Grid\n# =============================================================================\n\ndef find_image(image_id, search_dirs):\n    \"\"\"Search for an image file across multiple directories.\"\"\"\n    for d in search_dirs:\n        for ext in [\".jpg\", \".jpeg\", \".png\"]:\n            candidate = d / f\"{image_id}{ext}\"\n            if candidate.exists():\n                return candidate\n    return None\n\nsearch_dirs = [IMAGE_DIR]\nfor sub in [\"train\", \"val\", \"test\"]:\n    d = IMAGE_DIR / sub\n    if d.exists():\n        search_dirs.append(d)\n    d2 = IMAGE_DIR.parent / sub\n    if d2.exists():\n        search_dirs.append(d2)\n\nn_samples_per_class = 5\n\nif \"dx\" in df.columns and \"image_id\" in df.columns:\n    fig, axes = plt.subplots(len(CLASS_ORDER), n_samples_per_class, figsize=(3 * n_samples_per_class, 3 * len(CLASS_ORDER)))\n\n    for row_idx, cls in enumerate(CLASS_ORDER):\n        class_rows = df[df[\"dx\"] == cls].sample(\n            n=min(n_samples_per_class, len(df[df[\"dx\"] == cls])),\n            random_state=42\n        )\n        for col_idx in range(n_samples_per_class):\n            ax = axes[row_idx, col_idx]\n            if col_idx < len(class_rows):\n                image_id = str(class_rows.iloc[col_idx][\"image_id\"])\n                img_path = find_image(image_id, search_dirs)\n                if img_path is not None:\n                    ax.imshow(Image.open(img_path).convert(\"RGB\"))\n                    if col_idx == 0:\n                        ax.set_ylabel(f\"{cls.upper()}\\n({LABEL_MAP[cls]})\", fontsize=10, fontweight=\"bold\")\n                else:\n                    ax.text(0.5, 0.5, \"Not found\", ha=\"center\", va=\"center\")\n            ax.axis(\"off\")\n            if row_idx == 0:\n                ax.set_title(f\"Sample {col_idx + 1}\", fontsize=10)\n\n    plt.suptitle(\"Section 10: Sample Images per Diagnosis Class\", fontsize=16, fontweight=\"bold\", y=1.01)\n    plt.tight_layout()\n    plt.show()\n\n    # --- 10b. Mean image per class ---\n    print(\"\\nComputing mean image per class (up to 50 images each)...\")\n    fig, axes = plt.subplots(1, len(CLASS_ORDER), figsize=(3 * len(CLASS_ORDER), 3))\n    target_size = (224, 224)\n\n    for idx, cls in enumerate(CLASS_ORDER):\n        class_ids = df[df[\"dx\"] == cls][\"image_id\"].values\n        rng_cls = np.random.RandomState(42)\n        sample_ids = rng_cls.choice(class_ids, min(50, len(class_ids)), replace=False)\n\n        pixel_acc = np.zeros((*target_size, 3), dtype=np.float64)\n        count = 0\n        for img_id in sample_ids:\n            img_path = find_image(str(img_id), search_dirs)\n            if img_path is not None:\n                with Image.open(img_path) as im:\n                    arr = np.asarray(im.convert(\"RGB\").resize(target_size), dtype=np.float64)\n                    pixel_acc += arr\n                    count += 1\n\n        if count > 0:\n            mean_img = (pixel_acc / count).astype(np.uint8)\n            axes[idx].imshow(mean_img)\n        axes[idx].set_title(f\"{cls.upper()}\\n(n={count})\", fontsize=10, fontweight=\"bold\")\n        axes[idx].axis(\"off\")\n\n    plt.suptitle(\"Mean Image per Class (50-image average)\", fontsize=14, fontweight=\"bold\", y=1.02)\n    plt.tight_layout()\n    plt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# 11. Co-occurrence & Interaction Analysis\n# =============================================================================\n\nfig = plt.figure(figsize=(22, 14))\ngs = gridspec.GridSpec(2, 3, hspace=0.35, wspace=0.3)\n\n# --- 11a. Age-Sex interaction per class (strip plot) ---\nif \"age\" in df.columns and \"sex\" in df.columns:\n    ax1 = fig.add_subplot(gs[0, :2])\n    sns.stripplot(data=df.dropna(subset=[\"age\", \"sex\"]), x=\"dx\", y=\"age\", hue=\"sex\",\n                  order=CLASS_ORDER, dodge=True, alpha=0.4, size=3, palette=\"Set1\", ax=ax1)\n    sns.boxplot(data=df.dropna(subset=[\"age\", \"sex\"]), x=\"dx\", y=\"age\", hue=\"sex\",\n                order=CLASS_ORDER, dodge=True, palette=\"Set1\", ax=ax1,\n                boxprops=dict(alpha=0.5), showfliers=False, width=0.6)\n    handles, labels = ax1.get_legend_handles_labels()\n    # Keep only first set of labels (avoid duplication from box+strip)\n    n_sex = df[\"sex\"].nunique()\n    ax1.legend(handles[:n_sex], labels[:n_sex], title=\"Sex\", fontsize=9)\n    ax1.set_title(\"Age x Sex Interaction by Diagnosis\", fontweight=\"bold\")\n    ax1.set_xlabel(\"Diagnosis\")\n    ax1.set_ylabel(\"Age\")\n\n# --- 11b. Diagnosis type distribution ---\nif \"dx_type\" in df.columns:\n    ax2 = fig.add_subplot(gs[0, 2])\n    dx_type_counts = df[\"dx_type\"].value_counts()\n    ax2.pie(dx_type_counts.values, labels=dx_type_counts.index, autopct=\"%1.1f%%\",\n            colors=sns.color_palette(\"Pastel2\", n_colors=len(dx_type_counts)),\n            startangle=90, textprops={\"fontsize\": 9})\n    ax2.set_title(\"Diagnosis Confirmation Method\", fontweight=\"bold\")\n\n# --- 11c. Diagnosis type breakdown per class ---\nif \"dx_type\" in df.columns:\n    ax3 = fig.add_subplot(gs[1, :2])\n    dx_type_cross = pd.crosstab(df[\"dx\"], df[\"dx_type\"], normalize=\"index\").reindex(CLASS_ORDER) * 100\n    dx_type_cross.plot(kind=\"bar\", stacked=True, ax=ax3, colormap=\"tab20\", edgecolor=\"black\", linewidth=0.3)\n    ax3.set_title(\"Diagnosis Confirmation Method by Class (%)\", fontweight=\"bold\")\n    ax3.set_ylabel(\"Percent\")\n    ax3.set_xlabel(\"Diagnosis\")\n    ax3.legend(title=\"dx_type\", fontsize=8, bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n    ax3.tick_params(axis=\"x\", rotation=30)\n\n# --- 11d. Age-Localization heatmap (top localizations) ---\nif \"age\" in df.columns and \"localization\" in df.columns:\n    ax4 = fig.add_subplot(gs[1, 2])\n    top_locs = df[\"localization\"].value_counts().head(8).index\n    age_loc = df[df[\"localization\"].isin(top_locs)].groupby(\"localization\")[\"age\"].agg([\"mean\", \"std\", \"count\"])\n    age_loc = age_loc.sort_values(\"mean\", ascending=True)\n    ax4.barh(age_loc.index, age_loc[\"mean\"], xerr=age_loc[\"std\"], color=\"steelblue\",\n             edgecolor=\"black\", linewidth=0.5, capsize=3)\n    ax4.set_title(\"Mean Age by Localization (top 8)\", fontweight=\"bold\")\n    ax4.set_xlabel(\"Mean Age (+/- Std)\")\n\nplt.suptitle(\"Section 11: Co-occurrence & Interaction Analysis\", fontsize=16, fontweight=\"bold\", y=1.01)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# 12. Class Imbalance Strategy Recommendations\n# =============================================================================\n\nclass_counts_sorted = df[\"dx\"].value_counts().reindex(CLASS_ORDER)\ntotal = class_counts_sorted.sum()\nn_classes = len(CLASS_ORDER)\n\n# Compute various weighting schemes\ninv_freq = total / (n_classes * class_counts_sorted)\ninv_freq_norm = inv_freq / inv_freq.sum() * n_classes\n\nsqrt_inv_freq = np.sqrt(total / (n_classes * class_counts_sorted))\nsqrt_inv_freq_norm = sqrt_inv_freq / sqrt_inv_freq.sum() * n_classes\n\n# Effective number of samples (Class-Balanced Loss, Cui et al. 2019)\nbeta_values = [0.99, 0.999, 0.9999]\neffective_weights = {}\nfor beta in beta_values:\n    eff = (1 - beta) / (1 - beta ** class_counts_sorted)\n    eff_norm = eff / eff.sum() * n_classes\n    effective_weights[f\"CB (beta={beta})\"] = eff_norm.values\n\nweight_comparison = pd.DataFrame({\n    \"Class\": CLASS_ORDER,\n    \"Count\": class_counts_sorted.values,\n    \"Uniform\": [1.0] * n_classes,\n    \"Inv Freq\": inv_freq_norm.values.round(4),\n    \"Sqrt Inv Freq\": sqrt_inv_freq_norm.values.round(4),\n})\nfor k, v in effective_weights.items():\n    weight_comparison[k] = v.round(4)\n\nprint(\"Class Weighting Strategies Comparison:\")\ndisplay(weight_comparison)\n\n# Visualize\nfig, axes = plt.subplots(1, 2, figsize=(18, 6))\n\n# Weight comparison\nweight_cols = [\"Uniform\", \"Inv Freq\", \"Sqrt Inv Freq\"] + list(effective_weights.keys())\nx = np.arange(n_classes)\nwidth = 0.12\nfor i, col in enumerate(weight_cols):\n    axes[0].bar(x + i * width, weight_comparison[col], width, label=col, edgecolor=\"black\", linewidth=0.3)\naxes[0].set_xticks(x + width * len(weight_cols) / 2)\naxes[0].set_xticklabels([c.upper() for c in CLASS_ORDER])\naxes[0].set_title(\"Class Weight Comparison (Normalized)\", fontweight=\"bold\")\naxes[0].set_ylabel(\"Weight\")\naxes[0].legend(fontsize=8)\naxes[0].set_xlabel(\"Diagnosis\")\n\n# Effective samples per class at different betas\nfor beta in beta_values:\n    eff_samples = (1 - beta ** class_counts_sorted) / (1 - beta)\n    axes[1].plot(CLASS_ORDER, eff_samples.values, \"o-\", label=f\"beta={beta}\", linewidth=1.5, markersize=5)\naxes[1].plot(CLASS_ORDER, class_counts_sorted.values, \"s--\", color=\"black\", label=\"Actual count\", linewidth=1)\naxes[1].set_yscale(\"log\")\naxes[1].set_title(\"Effective Number of Samples (Class-Balanced Loss)\", fontweight=\"bold\")\naxes[1].set_ylabel(\"Effective Samples (log)\")\naxes[1].set_xlabel(\"Diagnosis\")\naxes[1].legend(fontsize=9)\n\nplt.suptitle(\"Section 12: Class Imbalance Strategies\", fontsize=16, fontweight=\"bold\", y=1.02)\nplt.tight_layout()\nplt.show()\n\n# Clinical cost analysis\nprint(\"\\nClinical Cost Analysis (for cost-sensitive loss):\")\nprint(\"-\" * 55)\nprint(f\"  {'Class':<8} {'Clinical Risk':<20} {'Suggested FN Cost'}\")\nprint(\"-\" * 55)\nclinical_costs = {\n    \"mel\": (\"HIGH - Lethal\", \"10x\"),\n    \"bcc\": (\"MODERATE - Invasive\", \"5x\"),\n    \"akiec\": (\"MODERATE - Pre-cancer\", \"3x\"),\n    \"bkl\": (\"LOW - Benign\", \"1x\"),\n    \"nv\": (\"LOW - Benign\", \"1x\"),\n    \"df\": (\"LOW - Benign\", \"1x\"),\n    \"vasc\": (\"LOW - Benign\", \"1x\"),\n}\nfor cls in CLASS_ORDER:\n    risk, cost = clinical_costs[cls]\n    print(f\"  {cls.upper():<8} {risk:<20} {cost}\")\nprint(\"-\" * 55)\nprint(\"\\nRecommendation: Use a 2-stage loss strategy:\")\nprint(\"  Stage 1: Focal Loss with class weights (learn general features)\")\nprint(\"  Stage 2: Cost-sensitive loss (penalize dangerous misses heavily)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 13. Key Findings & Modeling Implications\n\n### Dataset Characteristics\n- **Severe class imbalance**: NV (melanocytic nevus) dominates the dataset; rare classes (DF, VASC) are extremely underrepresented\n- **Multi-image lesions**: Multiple images can belong to the same lesion — **lesion-level splitting is mandatory** to prevent data leakage\n- **Missing metadata**: Age, sex, and localization have varying levels of missingness — imputation with missingness flags is appropriate\n\n### Statistical Insights\n- **Age is diagnostically informative**: ANOVA confirms significant age differences across diagnoses — certain lesions (e.g., melanoma, BCC) trend older\n- **Sex and localization carry signal**: Chi-square tests show significant associations with diagnosis — supports multi-modal fusion\n- **Diagnosis confirmation method varies**: Not all labels are histopathologically confirmed — consider label noise in modeling\n\n### Modeling Recommendations\n\n| Aspect | Recommendation |\n|--------|---------------|\n| **Data splitting** | Group by `lesion_id` to prevent leakage |\n| **Class imbalance** | Use focal loss + class-balanced sampling; 2-stage training with cost-sensitive loss |\n| **Metadata** | Include age, sex, localization as auxiliary features via late fusion or FiLM |\n| **Missing values** | Median imputation + explicit missingness flags (already implemented) |\n| **Augmentation** | Aggressive augmentation for minority classes; hair removal preprocessing |\n| **Clinical safety** | High false-negative penalty for MEL (10x) and BCC (5x) in cost-sensitive stage |\n| **Evaluation** | Use balanced accuracy, per-class recall, and confusion matrix — not just overall accuracy |\n| **Image preprocessing** | Color constancy (Shades of Gray) to reduce dermoscope variability |",
   "metadata": {}
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}